---
layout: post
title: Apuntes Clase 7
---

### MultiArms Bandits

**Explotación vs Exploración**
Ambas estrategias tienen riesgo. Por ejemplo, la explotacion nunca podria descubrir un nicho mejorn mientras que la exploracion puede ahuyentar al usuario. Se debe fijar en el contexto para decidir que camino de los anteriores tomar (ejemplo: con qué campo se está tratando?... ej medicina no es muy buena opción la exploración dada su naturaleza). O bien dar elección. La idea es intentar llegar a un balance.

**Conceptos claves de aprendizaje reforzado**

Environment : Mundo que reacciona a las accionas del agente. Ej: plataforma
State: Parámetros que describen estado actual. Ej: screen actual de un juego. Aqui se codifica el historial pasado del usuario
Agent: Algoritmo que ejecuta acciones basandose en el estado para maximizar recomenpensas a largo plazo
action: Qué elemento recomendar a cada usuario
step: Cambio de state
reward: Feedback del usuario
target:  Lo que el agente busca maximizar
policy: Qué accion tomar en cada caso. Lo que se aprendende dado un estado
episode: Conjunto de pasos que se define seugn el problema ej partida

**Multi Armed Bandits**

![multiarmsbandits](https://user-images.githubusercontent.com/63074428/206888442-f48ab60e-e9ea-4a8e-845b-13ba974899ab.PNG)

Agente debe decidir que maquina tirar en cada paso

Analogía Útil:
Máquinas tragamonedas -> Películas
Agente -> Sistema recomendador (puede ser 1 para todos o 1 para cada usuario)
Recomepnsa -> algún rating
Según eso va decidiendo qué película recomendar en el futuro. En verdad, el objetivo es ir aprendiendo la distribución desconocida de la recompensa de cada máquina.

**Como se mide el exito en RL ?**
Lo principal es intentar Maximizar la recompensa -> Es decir, maximir ya sea la suma $ o promedio $ o %correcto. etc

----
**Algoritmos para saber cuando explorar y cuando explotar**
![explorarexplotar](https://user-images.githubusercontent.com/63074428/206888531-db2122f9-6f39-496c-9230-5d79bf2734ff.png)


Estos algoritmos se basan en que "nunca" se deja de explorar (siempre existe probabilidad de explorar)

*Epsilon Greedy*

![foto eg](https://user-images.githubusercontent.com/63074428/206888548-7b9539fd-1b64-4d1b-a855-f7fd33a4dc9d.PNG)

En algoritmo original epsilon está fijo de forma predefinida.. pero existe Estrategia de epsilon decreciente .


*Thompson sampling *

intenta encontrar la distribucion de la prob de exito de cada acción basado en intentos pasados.
usa p(Q_t(a)|D_a)


*Upper confidence bound 19:13*
Se elije la acción cuyo intervlao de confianza abarque las mayores recomepensas 

![uppercb](https://user-images.githubusercontent.com/63074428/206888586-f860be24-3746-46d1-baa8-f51b74f68928.PNG)



RL:  Finalmente, lo que lo define es que sea un problema de decisión de markov --> maximizar recompensa a futuro


**Contextual Bandits**
Bandits que tienen en cuenta el contexto del usuario (ej Yahoo en su momento con LinUCB) 
